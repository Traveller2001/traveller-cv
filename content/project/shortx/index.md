---
title: Training the 10-Billion Parameter Yuan-1.0 LLM
summary: 'As a key member of the ASC23 team, I trained a 10B-level large language model using the DeepSpeed-Megatron framework, combining tensor, pipeline, and data parallelism. Our work won the **First Prize**.'
tags:
- Large Language Models
- High-Performance Computing
- ASC23
- Distributed Training
date: "2023-05-20"

# Optional external URL for project (e.g. website, source code, or demo)
external_link: ''

image:
  caption: 'Award'
  focal_point: Smart
---